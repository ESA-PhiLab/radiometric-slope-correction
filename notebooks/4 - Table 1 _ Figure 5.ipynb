{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4 - Table 1 & Figure 5.ipynb","provenance":[{"file_id":"1JKbuKT45CNGzGLAU8sA6bwT94mnfoGap","timestamp":1583397557643}],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1eS-x-nPr43dnyuHIub45Dityilyx472u","authorship_tag":"ABX9TyORaSkYUX70AEYz0qxfzVI1"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"UxlsCtf1wxZy","colab_type":"text"},"source":["# 1 - Install missing dependencies\n","\n","For the plotting of the Land Cover map we need the earthpy python package, that relies on the libspatialindex-dev lib."]},{"cell_type":"code","metadata":{"id":"fSd2goIbwybM","colab_type":"code","colab":{}},"source":["!apt-get -qq install -y libspatialindex-dev\n","!pip install -q --upgrade earthpy rasterio"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HWfNKKwC6W9z","colab_type":"text"},"source":["# 2 - Authenticate to Google Drive "]},{"cell_type":"code","metadata":{"id":"ZD2wpYUo6al0","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AtbO6VCjyQYT","colab_type":"text"},"source":["# 3 - Import necessary python libraries and mount google drive"]},{"cell_type":"code","metadata":{"id":"6BgiAXiUySOs","colab_type":"code","colab":{}},"source":["import os\n","import itertools\n","\n","import numpy as np\n","import pandas as pd \n","from scipy import stats\n","from scipy import optimize\n","\n","import rasterio as rio\n","from rasterio.windows import Window\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import earthpy.plot as ep\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l0_YOCs3ycr8","colab_type":"text"},"source":["# 4 - Read the data\n","\n","We read all bands into a dictionary, that hold the band names as key and the respective array as value. All data is cut to the same extent of 3000 x 3000 pixels.\n"]},{"cell_type":"code","metadata":{"id":"VTEKM564yXm5","colab_type":"code","colab":{}},"source":["list_of_layers = [\n","    'VV_gamma0', 'VH_gamma0',\n","    'VV_gamma0flat', 'VH_gamma0flat',\n","    'alpha_rRad', 'theta_liaRad', 'aspect', \n","    'layover', 'shadow',     \n","    'landcover']\n","\n","# paths to dem and model types\n","dem = 'SRTMGL1_003'\n","models = ['volume', 'surface']\n","buffers = [0]\n","\n","modelDict = {}\n","\n","# loop thorugh all combinations and put into dictionary\n","for model, buffer in itertools.product(models, buffers):\n","\n","    key = '{}_{}_buf_{}'.format(dem, model, buffer)\n","    # here we read all layers into a dictionary\n","    dataDict = {}\n","    for layer in list_of_layers:\n","        with rio.open('/content/drive/My Drive/slope_correction {}/{}.tif'.format(key, layer)) as src:\n","            print('Reading ' + layer)\n","            dataDict[layer] = np.nan_to_num(src.read(window=Window(0, 340, 3000, 3000)))[0]\n","            print(dataDict[layer].shape)\n","    \n","    # write respective dataDict to our model dict, where different models are stored\n","    modelDict[key] = dataDict"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ngKfW11t6PSg","colab_type":"text"},"source":["# 5 - Plotting functions"]},{"cell_type":"markdown","metadata":{"id":"G0URo63k9FtB","colab_type":"text"},"source":["## 5.1 - plot aspect against backscatter function"]},{"cell_type":"code","metadata":{"id":"yjyXLoafyo8q","colab_type":"code","colab":{}},"source":["def create_plot_aspect_against_backscatter(model, data, array, mask, stats_dict, outfile, gridsize):\n","    \n","    # getlayer info #\n","    polarisation = layer.split('_')[0]\n","    flat = layer.split('_')[1][-4:]\n","    if flat != 'flat':\n","        flat = False\n","    \n","    # fore and backslope lines\n","    look_angle = 82.30213964392819 # hardcoded, \n","    backslope = look_angle\n","    foreslope = backslope + 180\n","    vertical_y = np.linspace(-28, 8, 5)\n","    fs_x = 0 * vertical_y + foreslope\n","    bs_x = 0 * vertical_y + backslope\n","    \n","    # calculate mean line\n","    horizontal_x = np.linspace(0, 360, 10) \n","    mean_y = 0 * horizontal_x + stats_dict['mean']\n","    \n","    if not flat:\n","        y_label = r'$\\gamma^0$ [dB]'\n","    else:\n","        y_label = r'$\\gamma^0_f$ [dB]'\n","    \n","    # check for 0s in aspect and update mask\n","    data['aspect'][data['aspect'] == 0] = np.nan\n","    mask = mask & np.isfinite(data['aspect'])\n","    aspect_deg_masked = np.subtract(to_deg(data['aspect'][mask]), 180)\n","    aspect_deg_masked = to_deg(data['aspect'][mask])\n","\n","    # plot\n","    # surpress plotting, since we only want to save the files\n","    plt.ioff()\n","    X = sns.jointplot(aspect_deg_masked, array[mask], kind='hex', gridsize=(gridsize, gridsize))\n","    X.ax_joint.plot(horizontal_x, mean_y, 'k--', linewidth=.75)\n","    X.ax_joint.plot(fs_x, vertical_y, 'k--', linewidth=.75)\n","    X.ax_joint.plot(bs_x, vertical_y, 'k--', linewidth=.75)\n","    X.ax_joint.set_xlabel(r'Aspect angle $\\phi_s$ [deg]', fontsize=14)\n","    X.ax_joint.set_ylabel(y_label,  fontsize=14)\n","    X.ax_joint.set_ylim(-30, 10)\n","    #X.ax_joint.set_xlim(-190, 185)\n","    X.ax_joint.set_xlim(-10, 365)\n","    \n","    # add textbox with ampl, mean and sd\n","    props = dict(boxstyle='round', facecolor='lightgrey', alpha=0.5)\n","    textstr = '\\n'.join((\n","        r'$\\mathrm{A}=%.2f$' % (stats_dict['amplitude'], ),\n","        r'$\\mu=%.2f$' % (stats_dict['mean'], ),\n","        r'$\\sigma=%.2f$' % (stats_dict['sd'], )))\n","    X.ax_joint.text(290, -28,textstr, fontsize=10, bbox=props)\n","    \n","    # add title\n","    if not flat:\n","        title = '{} Backscatter modulation by slopes'.format(polarisation)\n","    else:\n","        title = '{} Backscatter after Model {}'.format(polarisation, model)\n","        \n","    plt.suptitle(title, x=0.45, y=1.02, fontweight='bold', fontsize=14)\n","    \n","    # save\n","    plt.savefig(outfile, bbox_inches='tight', pad_inches=0.5)\n","    plt.close()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tQxejHE09RmS","colab_type":"text"},"source":["## 5.2 - plot slope steepness against backscatter function"]},{"cell_type":"code","metadata":{"id":"Lvts98T89R2y","colab_type":"code","colab":{}},"source":["def create_plot_alpha_against_backscatter(model, data, array, mask, stats_dict, outfile, gridsize):\n","    \n","    # getlayer info #\n","    polarisation = layer.split('_')[0]\n","    flat = layer.split('_')[1][-4:]\n","    if flat != 'flat':\n","        flat = False\n","    \n","    # calculate mean line\n","    horizontal_x = np.linspace(-40, 40, 10) \n","    mean_y = 0 * horizontal_x + stats_dict['mean']\n","    \n","    if not flat:\n","        y_label = r'$\\gamma^0$ [dB]'\n","    else:\n","        y_label = r'$\\gamma^0_f$ [dB]'\n","    \n","    # check for 0s in aspect and update mask\n","    data['alpha_rRad'][data['alpha_rRad'] == 0] = np.nan\n","    mask = mask & np.isfinite(data['alpha_rRad'])\n","    alpha_deg_masked = to_deg(data['alpha_rRad'][mask])\n","\n","    # plot\n","    # surpress plotting, since we only want to save the files\n","    plt.ioff()\n","    X = sns.jointplot(alpha_deg_masked, array[mask], kind='hex', gridsize=(gridsize, gridsize))\n","    X.ax_joint.plot(horizontal_x, mean_y, 'k--', linewidth=.75)\n","    X.ax_joint.set_xlabel(r'Slope Steepness in range $\\alpha$ [deg]', fontsize=14)\n","    X.ax_joint.set_ylabel(y_label, fontsize=14)\n","    X.ax_joint.set_ylim(-30, 10)\n","    X.ax_joint.set_xlim(-45, 45)\n","    \n","    # add textbox with ampl, mean and sd\n","    props = dict(boxstyle='round', facecolor='lightgrey', alpha=0.5)\n","    textstr = '\\n'.join((\n","        r'$\\mathrm{s}=%.2f$' % (stats_dict['slope'], ),\n","        r'$\\mu=%.2f$' % (stats_dict['mean'], ),\n","        r'$\\sigma=%.2f$' % (stats_dict['sd'], )))\n","    X.ax_joint.text(20, -28,textstr, fontsize=10, bbox=props)\n","    \n","    # add title\n","    if not flat:\n","        title = '{} Backscatter modulation by slopes'.format(polarisation)\n","    else:\n","        title = '{} Backscatter after Model {}'.format(polarisation, model)\n","        \n","    plt.suptitle(title, x=0.45, y=1.02, fontweight='bold', fontsize=14)\n","    \n","    # save\n","    plt.savefig(outfile, bbox_inches='tight', pad_inches=0.5)\n","    plt.close()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pg0qQvwG9i0g","colab_type":"text"},"source":["## 5.3 - plot lia against backscatter function"]},{"cell_type":"code","metadata":{"id":"OaJVHtQW9i9l","colab_type":"code","colab":{}},"source":["def create_plot_lia_against_backscatter(model, data, array, mask, stats_dict, outfile, gridsize):\n","    \n","    \n","    # getlayer info #\n","    polarisation = layer.split('_')[0]\n","    flat = layer.split('_')[1][-4:]\n","    if flat != 'flat':\n","        flat = False\n","    \n","    # calculate mean line\n","    horizontal_x = np.linspace(0, 90, 10) \n","    mean_y = 0 * horizontal_x + stats_dict['mean']\n","    \n","    if not flat:\n","        y_label = r'$\\gamma^0$ [dB]'\n","    else:\n","        y_label = r'$\\gamma^0_f$ [dB]'\n","    \n","    # check for 0s in aspect and update mask\n","    data['theta_liaRad'][data['theta_liaRad'] == 0] = np.nan\n","    mask = mask & np.isfinite(data['theta_liaRad'])\n","    theta_deg_masked = to_deg(data['theta_liaRad'][mask])\n","\n","    # plot\n","    # surpress plotting, since we only want to save the files\n","    plt.ioff()\n","    X = sns.jointplot(theta_deg_masked, array[mask], kind='hex', gridsize=(gridsize, gridsize))\n","    X.ax_joint.plot(horizontal_x, mean_y, 'k--', linewidth=.75)\n","    X.ax_joint.set_xlabel(r'Local Incidence Angle $\\theta$ [deg]', fontsize=14)\n","    X.ax_joint.set_ylabel(y_label, fontsize=14)\n","    X.ax_joint.set_ylim(-30, 10)\n","    X.ax_joint.set_xlim(-10, 100)\n","    \n","    # add textbox with ampl, mean and sd\n","    props = dict(boxstyle='round', facecolor='lightgrey', alpha=0.5)\n","    textstr = '\\n'.join((\n","        #r'$\\mathrm{s}=%.2f$' % (stats_dict['slope'], ),\n","        r'$\\mu=%.2f$' % (stats_dict['mean'], ),\n","        r'$\\sigma=%.2f$' % (stats_dict['sd'], )))\n","    X.ax_joint.text(-5, -28,textstr, fontsize=10, bbox=props)\n","    \n","    # add title\n","    if not flat:\n","        title = '{} Backscatter modulation by slopes'.format(polarisation)\n","    else:\n","        title = '{} Backscatter after Model {}'.format(polarisation, model)\n","        \n","    plt.suptitle(title, x=0.45, y=1.02, fontweight='bold', fontsize=14)\n","    \n","    # save\n","    plt.savefig(outfile, bbox_inches='tight', pad_inches=0.5)\n","    plt.close()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iItiSMFf7yRG","colab_type":"text"},"source":["# 6 - Statistics functions"]},{"cell_type":"code","metadata":{"id":"Y9cQYDg8znMo","colab_type":"code","colab":{}},"source":["# from rad to degree\n","def to_deg(rad):\n","    return np.multiply(rad,  np.divide(180,np.pi))\n","\n","# sin function or curve fitting\n","def sin_func(x, a, b, c):\n","    return a * np.sin(x + b) - c\n","\n","def tf_stats(data, array, layer, lc_class, mask):\n","    \n","    #------------------------------------------------\n","    # slope calculation\n","    \n","    # mask alpha angle\n","    data['alpha_rRad'][data['alpha_rRad'] == 0] = np.nan\n","    mask_alpha = mask & np.isfinite(data['alpha_rRad'])\n","    alpha_deg_masked = np.subtract(to_deg(data['alpha_rRad'][mask_alpha]), 180)\n","    \n","    # mask out nans\n","    x = array.copy()\n","    x[~mask_alpha] = np.nan\n","    x = x[np.logical_not(np.isnan(x))] \n","    y = alpha_deg_masked\n","    y = y[np.logical_not(np.isnan(y))]\n","    \n","    # lin-regression\n","    slope, intercept, r_value, p_value, std_err = stats.linregress(y, x)        \n","    #------------------------------------------------\n","    \n","    #------------------------------------------------\n","    # amplitude calculation\n","    \n","    # mask aspect 0s and nans\n","    data['aspect'][data['aspect'] == 0] = np.nan\n","    mask_aspect = mask & np.isfinite(data['aspect'])\n","    aspect_deg_masked = np.subtract(data['aspect'][mask_aspect], np.pi)\n","    \n","    # mask out nans\n","    x = array.copy()\n","    x[~mask_aspect] = np.nan\n","    x = x[np.logical_not(np.isnan(x))]\n","    y = aspect_deg_masked\n","    y = y[np.logical_not(np.isnan(y))]\n","   \n","    # curve fitting\n","    params, params_covariance = optimize.curve_fit(sin_func, y, x)\n","    amp = params[0]\n","    #------------------------------------------------\n","\n","    # mean, sd\n","    mean = np.nanmean(array[mask_alpha])\n","    std = np.nanstd(array[mask_alpha])\n","    \n","    # create final dictionary\n","    stat_dict = {'lc_class': lc_class, \n","                 'layer': layer, \n","                 'count': np.sum(mask), \n","                 'mean': mean, \n","                 'sd': std, \n","                 'slope': slope, \n","                 'amplitude': np.abs(amp)\n","                }\n","    \n","    return stat_dict"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0iuOOizd-n_S","colab_type":"text"},"source":["# 7 - Create figures and statistics dataframe or each model and land cover class"]},{"cell_type":"code","metadata":{"id":"UhxhmzM71IBf","colab_type":"code","colab":{}},"source":["# class names\n","legend_entries = [\"Build-up\", \"Flat sealed surfaces\", \"Permament soil\", \"Bare rock and screes\", \n","                  \"Water\", \"Snow and ice\", \"Trees - broad-leaved\", \"Tree - coniferous\", \"Bushes and shrubs\", \"Herbacous periodically\",\n","                  \"Herbacous permanent - low productivity\", \"Herbacous permanent - high productivity\", \"Reeds\"]\n","# class values\n","ras_values = [11, 12, 31, 32, 60, 70, 91, 93, 100, 122, 125, 126, 130]\n","\n","# reduced list of class names\n","legend_entries = [\"Bare rock and screes\", \"Trees - broad-leaved\", \n","                  \"Tree - coniferous\", \"Bushes and shrubs\", \n","                  \"Herbacous periodically\", \n","                  \"Herbacous permanent - high productivity\", \"Reeds\"]\n","# redued list of class values\n","ras_values = [32, 91, 93, 100, 122,  126, 130]\n","\n","# prepare columns for new dataframe\n","df_cols = ['lc_class', 'layer', 'count', 'mean', 'sd', 'slope', 'amplitude']\n","\n","# paths to dem and model types and buffer\n","dem = 'SRTMGL1_003'\n","models = ['surface', 'volume']\n","buffers = [0] \n","\n","# loop through different models and buffers\n","for model, buffer in itertools.product(models, buffers):\n","    \n","    # get respective arrays within the model/datadict\n","    key = '{}_{}_buf_{}'.format(dem, model, buffer)\n","    dataDict = modelDict[key]\n","    \n","    print(' INFO: Creating figures and stats for {} {} with buffer {}.'\n","      .format(dem, model, buffer)\n","    )\n","    # create empty dataframe for statistics\n","    df_stats = pd.DataFrame(columns=df_cols)\n","    \n","    # crate outdirectory where plots and stats will be saved\n","    outdir = '/content/drive/My Drive/slope_correction/pictures/{}/'.format(key)\n","    os.makedirs(outdir, exist_ok=True)\n","    \n","    # loop through classes and respective raster values file\n","    for legend_entry, ras_value in zip(legend_entries, ras_values):\n","        \n","        # set raster value respective to class\n","        print(' INFO: Analysing {} with raster value {}.'\n","          .format(legend_entry, ras_value)\n","        )\n","\n","        # loop thorugh different corrected and uncorrected layers\n","        for layer in ['VV_gamma0', 'VV_gamma0flat', 'VH_gamma0', 'VH_gamma0flat']:\n","                        \n","            # create combined Land Cover and Layover/Shadow mask\n","            valid_data_mask = (\n","                [dataDict['landcover'] == ras_value] & \n","                (dataDict['layover'] > 0) & \n","                (dataDict['shadow'] > 0)\n","            )[0] \n","\n","            # apply this mask and add valid data mask of backscatter array \n","            array = dataDict[layer].copy()\n","            array[array == 0] = np.nan\n","            mask = valid_data_mask & np.isfinite(array)\n","\n","            # set everything else to nan\n","            array[~mask] = np.nan\n","\n","            # for some classes array might be empty, so we add an if\n","            if True in np.unique(np.isfinite(array)):\n","                \n","                # calculate stats\n","                stats_dict = tf_stats(\n","                    dataDict.copy(), array, layer, legend_entry, mask\n","                )\n","                stats_dict['lc_class_code'] =  ras_value\n","\n","                # and put into pandas dataframe\n","                df = pd.DataFrame([stats_dict], columns=stats_dict.keys())\n","                df_stats = df_stats.append(stats_dict, ignore_index=True)\n","\n","                # plotting\n","                gridsize=100\n","                model_nr = '1' if model == 'volume' else '2'\n","\n","                create_plot_aspect_against_backscatter(\n","                    model_nr, dataDict.copy(), array, mask, stats_dict, \n","                    '{}/{}_{}_vs_aspect.png'.format(outdir, legend_entry, layer), \n","                    gridsize\n","                )\n","\n","                create_plot_alpha_against_backscatter(\n","                    model_nr, dataDict.copy(), array, mask, stats_dict, \n","                    '{}/{}_{}_vs_slope.png'.format(outdir, legend_entry, layer), \n","                    gridsize\n","                )\n","\n","                create_plot_lia_against_backscatter(\n","                    model_nr, dataDict.copy(), array, mask, stats_dict, \n","                    '{}/{}_{}_vs_LIA.png'.format(outdir, legend_entry, layer), \n","                    gridsize\n","                )\n","\n","        # save the complete stas dataframe to pickle\n","        df_stats.reset_index()\n","        df_stats.to_pickle('{}/stats.pickle'.format(outdir)) "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RtT_Bpg9IpYV","colab_type":"text"},"source":["# 8 - re-organise statistics dataframe for creation of Table 1"]},{"cell_type":"code","metadata":{"id":"ulDA5czHAPx9","colab_type":"code","colab":{}},"source":["df_stats_dict = {}\n","\n","# paths to dem and model types\n","dem = 'SRTMGL1_003'\n","models = ['surface', 'volume'] \n","\n","# this is for the concatenation of VV and VH stats\n","concat_cols = ['lc_class', 'lc class code', 'VV', 'Pixel count', 'VV mean', 'VV SD', 'VV slope', 'VV amp', \n","                'VH', 'VH mean', 'VH SD', 'VH slope', 'VH amp']\n","\n","# create empty list for adding all stats within the for loop\n","df_con_merged = []\n","\n","# loop through different dems and models\n","for model in models:\n","\n","    # get respective arrays within the datadict\n","    key = '{}_{}_buf_0'.format(dem, model)\n","    \n","    # read each df into the dictionary\n","    outdir = '/content/drive/My Drive/slope_correction/pictures/{}/'.format(key)\n","    df_stats = pd.read_pickle('{}/stats.pickle'.format(outdir))\n","    \n","    # split into vv and vh\n","    df_vv = df_stats[df_stats['layer'].str.contains('VV')].reset_index().rename(columns={'layer': 'VV-pol'})\n","    df_vh = df_stats[df_stats['layer'].str.contains('VH')].reset_index().rename(columns={'layer': 'VH-pol'})\n","    \n","    # concat vv and vh columns\n","    df_con = pd.concat([df_vv, df_vh], axis=1, ignore_index=True)\n","    \n","    # rename columns\n","    df_con.columns = ['i_2', 'lc_class', 'VV', 'Pixel count', 'VV mean', 'VV SD', 'VV slope', 'VV amp', 'lc class code',\n","                      'i_3', 'lc_class_2', 'VH', 'VH count', 'VH mean', 'VH SD', 'VH slope', 'VH amp', 'lc class code_2']\n","    \n","    # subset columns\n","    df_con = df_con[['lc_class', 'lc class code', 'VV', 'Pixel count', 'VV mean', 'VV SD', 'VV slope', 'VV amp', \n","                     'VH', 'VH mean', 'VH SD', 'VH slope', 'VH amp']]\n","    \n","    # rename the layer names \n","    df_con['VV'] = df_con['VV'].str.replace('VV_gamma0flat', '{}'.format(key))\n","    df_con['VH'] = df_con['VH'].str.replace('VH_gamma0flat', '{}'.format(key))\n","    \n","    df_con['VV'] = df_con['VV'].str.replace('VV_gamma0', 'Original')\n","    df_con['VH'] = df_con['VH'].str.replace('VH_gamma0', 'Original')\n","    \n","    # merge to existent dfs\n","    df_con_merged.append(df_con)\n","\n","\n","# bring all the data together\n","appended_data = pd.concat(df_con_merged)\n","\n","# exclude marginal classes\n","appended_data = appended_data[appended_data['Pixel count'] >= 100000]\n","appended_data = appended_data[['lc_class', 'VV', 'VV mean', 'VV SD', 'VV slope', 'VV amp', 'VH mean', 'VH SD', 'VH slope', 'VH amp']]\n","\n","# rename model names\n","appended_data['VV'] = appended_data['VV'].str.replace('SRTMGL1_003_volume_buf_0', 'Model I')\n","appended_data['VV'] = appended_data['VV'].str.replace('SRTMGL1_003_surface_buf_0', 'Model II')\n","\n","# remove double entries for original data and reindex \n","appended_data.drop_duplicates(subset=['lc_class', 'VV'], keep='first', inplace=True)\n","appended_data = appended_data.set_index(['lc_class', 'VV']).sort_values(['lc_class', 'VV'], ascending=False)\n","\n","# rename model column\n","appended_data"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4XjtcNTPM8Wq","colab_type":"text"},"source":["# 9 - Export statistics as latex table "]},{"cell_type":"code","metadata":{"id":"UXMmF5_lM8fl","colab_type":"code","colab":{}},"source":["with open('/content/drive/My Drive/slope_correction/table_all.tex', 'w') as tf:\n","     tf.write(appended_data.to_latex(columns = ['VV mean', 'VV SD', 'VV slope', 'VV amp', 'VH mean', 'VH SD', 'VH slope', 'VH amp'], index=True, bold_rows=True, escape=True, float_format=lambda x: '%10.3f' % x))"],"execution_count":0,"outputs":[]}]}